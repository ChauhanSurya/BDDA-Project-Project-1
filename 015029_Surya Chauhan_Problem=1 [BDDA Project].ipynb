{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Surya = SparkSession.builder.appName(\"Text Classification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "TC = spark.read.csv(\"Corona_NLP_train.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|                3799|               48751|              London|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...| Positive|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...| Positive|\n",
      "|                3802|               48754|                null|          16-03-2020|My food stock is ...|     null|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|     null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|     null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|     null|\n",
      "|                3803|               48755|                null|          16-03-2020|Me, ready to go a...|     null|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|     null|\n",
      "|#CoronavirusFranc...|  Extremely Negative|                null|                null|                null|     null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TC.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the top 10 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show columns in the Datasets \n",
    "TC.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68046"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName             4\n",
       "ScreenName       12417\n",
       "Location         33799\n",
       "TweetAt          26311\n",
       "OriginalTweet    26663\n",
       "Sentiment        39429\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TC.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the data sets contains a lot of null values. We must remove these null values or they can affect the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC = TC.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName         0\n",
       "ScreenName       0\n",
       "Location         0\n",
       "TweetAt          0\n",
       "OriginalTweet    0\n",
       "Sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TC.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the null values of the data set has been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22358"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+------------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|         Sentiment|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...|          Positive|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...|          Positive|\n",
      "|    3804|     48756|ÜT: 36.319708,-82...|16-03-2020|As news of the re...|          Positive|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...|          Positive|\n",
      "|    3807|     48759|     Atlanta, GA USA|16-03-2020|Due to COVID-19 o...|          Positive|\n",
      "|    3808|     48760|    BHAVNAGAR,GUJRAT|16-03-2020|For corona preven...|          Negative|\n",
      "|    3809|     48761|      Makati, Manila|16-03-2020|All month there h...|           Neutral|\n",
      "|    3811|     48763|          Horningsea|16-03-2020|#horningsea is a ...|Extremely Positive|\n",
      "|    3818|     48770|          Denver, CO|16-03-2020|For those who are...|          Positive|\n",
      "|    3819|     48771|southampton soxx xxx|16-03-2020|with 100  nations...|Extremely Negative|\n",
      "|    3823|     48775|    Downstage centre|16-03-2020|@10DowningStreet ...|          Negative|\n",
      "|    3824|     48776|              London|16-03-2020|UK #consumer poll...|Extremely Positive|\n",
      "|    3825|     48777|      Ketchum, Idaho|16-03-2020|In preparation fo...|          Negative|\n",
      "|    3826|     48778| Everywhere You Are!|16-03-2020|This morning I te...|Extremely Negative|\n",
      "|    3834|     48786|             Sverige|16-03-2020|Went to the super...|           Neutral|\n",
      "|    3836|     48788|              Canada|16-03-2020|Worried about the...|          Positive|\n",
      "|    3838|     48790|       United States|16-03-2020|Now I can go to t...|          Positive|\n",
      "|    3841|     48793|             Houston|16-03-2020|CHECK VIDEO ?? ht...|Extremely Negative|\n",
      "|    3842|     48794|Vancouver, Britis...|16-03-2020|Breaking Story: O...|           Neutral|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TC.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set after removing the null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC = TC.withColumn('Tweet_length', length(TC['OriginalTweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|Tweet_length|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|         111|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...| Positive|         237|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|         131|\n",
      "|    3804|     48756|ÜT: 36.319708,-82...|16-03-2020|As news of the re...| Positive|         249|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...| Positive|         184|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TC.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['Positive','Negative','Neutral','Extremely Positive','Extremely Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = TC.filter(TC.Sentiment.isin(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         Sentiment|\n",
      "+------------------+\n",
      "|Extremely Negative|\n",
      "|           Neutral|\n",
      "|          Positive|\n",
      "|          Negative|\n",
      "|Extremely Positive|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AB.select('Sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AB.select('Sentiment').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|         Sentiment|count|\n",
      "+------------------+-----+\n",
      "|Extremely Negative| 2889|\n",
      "|           Neutral| 4128|\n",
      "|          Positive| 6041|\n",
      "|          Negative| 5261|\n",
      "|Extremely Positive| 3491|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AB.groupby('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data is grouped on the basis of the sentiments of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Location|count|\n",
      "+--------------------+-----+\n",
      "|                 ...|    1|\n",
      "| Mumbai, Maharashtra|    3|\n",
      "| Brisbane, Australia|    4|\n",
      "|West Woofle-Dust ...|    1|\n",
      "|   St Petersburg, FL|    7|\n",
      "| All across Michigan|    1|\n",
      "|     Northumberland |    1|\n",
      "|     stoke on trent |    1|\n",
      "|some where around...|    1|\n",
      "|           Bangalore|   19|\n",
      "|           Norn Iron|    1|\n",
      "|Horsham, Pennsylv...|    1|\n",
      "|       Shimla  India|    1|\n",
      "|Ferrara, Emilia R...|    1|\n",
      "|      Luton, England|    1|\n",
      "|              Heaven|    1|\n",
      "|       St George, UT|    1|\n",
      "|Just to the left ...|    1|\n",
      "|           Worcester|    2|\n",
      "|      Nellore/Canada|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AB.groupby('Location').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum number of tweets are from Bangalore Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+------------------+------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|         Sentiment|Tweet_length|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|         111|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...|          Positive|         237|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...|          Positive|         131|\n",
      "|    3804|     48756|ÜT: 36.319708,-82...|16-03-2020|As news of the re...|          Positive|         249|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...|          Positive|         184|\n",
      "|    3807|     48759|     Atlanta, GA USA|16-03-2020|Due to COVID-19 o...|          Positive|         280|\n",
      "|    3808|     48760|    BHAVNAGAR,GUJRAT|16-03-2020|For corona preven...|          Negative|         267|\n",
      "|    3809|     48761|      Makati, Manila|16-03-2020|All month there h...|           Neutral|         276|\n",
      "|    3811|     48763|          Horningsea|16-03-2020|#horningsea is a ...|Extremely Positive|         278|\n",
      "|    3818|     48770|          Denver, CO|16-03-2020|For those who are...|          Positive|         184|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AB.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21810, 7)\n"
     ]
    }
   ],
   "source": [
    "print((AB.count(),len(AB.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Binarizer',\n",
       " 'BucketedRandomProjectionLSH',\n",
       " 'BucketedRandomProjectionLSHModel',\n",
       " 'Bucketizer',\n",
       " 'ChiSqSelector',\n",
       " 'ChiSqSelectorModel',\n",
       " 'CountVectorizer',\n",
       " 'CountVectorizerModel',\n",
       " 'DCT',\n",
       " 'ElementwiseProduct',\n",
       " 'FeatureHasher',\n",
       " 'HasAggregationDepth',\n",
       " 'HasBlockSize',\n",
       " 'HasCheckpointInterval',\n",
       " 'HasCollectSubModels',\n",
       " 'HasDistanceMeasure',\n",
       " 'HasElasticNetParam',\n",
       " 'HasFeaturesCol',\n",
       " 'HasFitIntercept',\n",
       " 'HasHandleInvalid',\n",
       " 'HasInputCol',\n",
       " 'HasInputCols',\n",
       " 'HasLabelCol',\n",
       " 'HasLoss',\n",
       " 'HasMaxIter',\n",
       " 'HasNumFeatures',\n",
       " 'HasOutputCol',\n",
       " 'HasOutputCols',\n",
       " 'HasParallelism',\n",
       " 'HasPredictionCol',\n",
       " 'HasProbabilityCol',\n",
       " 'HasRawPredictionCol',\n",
       " 'HasRegParam',\n",
       " 'HasRelativeError',\n",
       " 'HasSeed',\n",
       " 'HasSolver',\n",
       " 'HasStandardization',\n",
       " 'HasStepSize',\n",
       " 'HasThreshold',\n",
       " 'HasThresholds',\n",
       " 'HasTol',\n",
       " 'HasValidationIndicatorCol',\n",
       " 'HasVarianceCol',\n",
       " 'HasWeightCol',\n",
       " 'HashingTF',\n",
       " 'IDF',\n",
       " 'IDFModel',\n",
       " 'Imputer',\n",
       " 'ImputerModel',\n",
       " 'IndexToString',\n",
       " 'Interaction',\n",
       " 'JavaEstimator',\n",
       " 'JavaMLReadable',\n",
       " 'JavaMLWritable',\n",
       " 'JavaModel',\n",
       " 'JavaParams',\n",
       " 'JavaTransformer',\n",
       " 'MaxAbsScaler',\n",
       " 'MaxAbsScalerModel',\n",
       " 'MinHashLSH',\n",
       " 'MinHashLSHModel',\n",
       " 'MinMaxScaler',\n",
       " 'MinMaxScalerModel',\n",
       " 'NGram',\n",
       " 'Normalizer',\n",
       " 'OneHotEncoder',\n",
       " 'OneHotEncoderModel',\n",
       " 'PCA',\n",
       " 'PCAModel',\n",
       " 'Param',\n",
       " 'Params',\n",
       " 'PolynomialExpansion',\n",
       " 'QuantileDiscretizer',\n",
       " 'RFormula',\n",
       " 'RFormulaModel',\n",
       " 'RegexTokenizer',\n",
       " 'RobustScaler',\n",
       " 'RobustScalerModel',\n",
       " 'SQLTransformer',\n",
       " 'SparkContext',\n",
       " 'StandardScaler',\n",
       " 'StandardScalerModel',\n",
       " 'StopWordsRemover',\n",
       " 'StringIndexer',\n",
       " 'StringIndexerModel',\n",
       " 'Tokenizer',\n",
       " 'TypeConverters',\n",
       " 'VectorAssembler',\n",
       " 'VectorIndexer',\n",
       " 'VectorIndexerModel',\n",
       " 'VectorSizeHint',\n",
       " 'VectorSlicer',\n",
       " 'Word2Vec',\n",
       " 'Word2VecModel',\n",
       " '_BucketedRandomProjectionLSHParams',\n",
       " '_ChiSqSelectorParams',\n",
       " '_CountVectorizerParams',\n",
       " '_IDFParams',\n",
       " '_ImputerParams',\n",
       " '_LSH',\n",
       " '_LSHModel',\n",
       " '_LSHParams',\n",
       " '_MaxAbsScalerParams',\n",
       " '_MinMaxScalerParams',\n",
       " '_OneHotEncoderParams',\n",
       " '_PCAParams',\n",
       " '_RFormulaParams',\n",
       " '_RobustScalerParams',\n",
       " '_StandardScalerParams',\n",
       " '_StringIndexerParams',\n",
       " '_VectorIndexerParams',\n",
       " '_Word2VecParams',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_convert_to_vector',\n",
       " '_jvm',\n",
       " 'basestring',\n",
       " 'ignore_unicode_prefix',\n",
       " 'inherit_doc',\n",
       " 'keyword_only',\n",
       " 'since',\n",
       " 'sys']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark.ml.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stages of Pipeline\n",
    "tokenizer = Tokenizer(inputCol='OriginalTweet', outputCol='token_text')\n",
    "stopwords_remover = StopWordsRemover(inputCol='token_text', outputCol='stop_token')\n",
    "vectorizer = CountVectorizer(inputCol='stop_token', outputCol='RawTweets')\n",
    "idf = IDF(inputCol='RawTweets', outputCol='Tweets')\n",
    "\n",
    "#Converting Labels to Numeric\n",
    "labeltonum=StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD = VectorAssembler(inputCols=[\"Tweets\",\"Tweet_length\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "nb=NaiveBayes()\n",
    "rf=RandomForestClassifier(numTrees=100)\n",
    "dtc=DecisionTreeClassifier(maxDepth=15)\n",
    "\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labeltonum, tokenizer, stopwords_remover, vectorizer, idf, CD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_43a3881c98d7"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='Pipeline_43a3881c98d7', name='stages', doc='a list of pipeline stages')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "EF = pipeline.fit(AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "GH = EF.transform(AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+---------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|Tweet_length|label|          token_text|          stop_token|           RawTweets|              Tweets|            features|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|         111|  2.0|[@menyrbie, @phil...|[@menyrbie, @phil...|(66313,[14329,347...|(66313,[14329,347...|(66314,[14329,347...|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...| Positive|         237|  0.0|[advice, talk, to...|[advice, talk, ne...|(66313,[14,15,133...|(66313,[14,15,133...|(66314,[14,15,133...|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|         131|  0.0|[coronavirus, aus...|[coronavirus, aus...|(66313,[7,15,35,7...|(66313,[7,15,35,7...|(66314,[7,15,35,7...|\n",
      "|    3804|     48756|ÜT: 36.319708,-82...|16-03-2020|As news of the re...| Positive|         249|  0.0|[as, news, of, th...|[news, regions, ...|(66313,[7,8,31,47...|(66313,[7,8,31,47...|(66314,[7,8,31,47...|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...| Positive|         184|  0.0|[\"cashier, at, gr...|[\"cashier, grocer...|(66313,[4,6,18,60...|(66313,[4,6,18,60...|(66314,[4,6,18,60...|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GH.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "GH = GH.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  2.0|(66314,[14329,347...|\n",
      "|  0.0|(66314,[14,15,133...|\n",
      "|  0.0|(66314,[7,15,35,7...|\n",
      "|  0.0|(66314,[7,8,31,47...|\n",
      "|  0.0|(66314,[4,6,18,60...|\n",
      "|  0.0|(66314,[1,6,7,14,...|\n",
      "|  1.0|(66314,[11,14,15,...|\n",
      "|  2.0|(66314,[51,73,151...|\n",
      "|  3.0|(66314,[14,15,24,...|\n",
      "|  0.0|(66314,[3,7,22,39...|\n",
      "|  4.0|(66314,[1,3,9,11,...|\n",
      "|  1.0|(66314,[3,21,44,7...|\n",
      "|  3.0|(66314,[10,35,50,...|\n",
      "|  1.0|(66314,[3,7,22,32...|\n",
      "|  4.0|(66314,[1,8,11,37...|\n",
      "|  2.0|(66314,[5,47,51,6...|\n",
      "|  0.0|(66314,[7,12,24,2...|\n",
      "|  0.0|(66314,[5,19,20,4...|\n",
      "|  4.0|(66314,[0,3,12,21...|\n",
      "|  2.0|(66314,[0,8,14,15...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GH.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting Dataset into Train and Test Data\n",
    "(traintc, testtc) = GH.randomSplit((0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictNB = nb.fit(traintc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictRF= rf.fit(traintc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_results = PredictNB.transform(testtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_results = PredictRF.transform(testtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(66314,[0,1,2,8,4...|[-1266.1194558574...|[1.62972945933352...|       4.0|\n",
      "|  0.0|(66314,[0,1,2,12,...|[-1131.7293835562...|[0.05141633318800...|       3.0|\n",
      "|  0.0|(66314,[0,1,2,13,...|[-1693.7115663206...|[7.59525903638559...|       3.0|\n",
      "|  0.0|(66314,[0,1,2,16,...|[-1180.1247174083...|[0.99993722733317...|       0.0|\n",
      "|  0.0|(66314,[0,1,2,25,...|[-2008.1332073200...|[1.0,6.3180101352...|       0.0|\n",
      "|  0.0|(66314,[0,1,3,4,1...|[-1389.6572124391...|[4.61765690218920...|       2.0|\n",
      "|  0.0|(66314,[0,1,3,5,1...|[-818.96975489271...|[0.33814644066027...|       3.0|\n",
      "|  0.0|(66314,[0,1,3,10,...|[-1909.5191209775...|[1.95112279013779...|       1.0|\n",
      "|  0.0|(66314,[0,1,3,31,...|[-1517.8262106313...|[5.00956386455401...|       1.0|\n",
      "|  0.0|(66314,[0,1,4,6,8...|[-1308.1188259731...|[2.67961875480927...|       1.0|\n",
      "|  0.0|(66314,[0,1,4,6,8...|[-918.68274579619...|[0.99999999996146...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[-1106.5853313568...|[1.88795311769257...|       3.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[-1517.2414097555...|[2.53666614683607...|       4.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[-1401.3681501814...|[3.87159013479508...|       3.0|\n",
      "|  0.0|(66314,[0,1,4,6,5...|[-542.27154888444...|[1.12414466366658...|       4.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[-917.34680072016...|[0.99924333914013...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,12,...|[-2133.0540492735...|[4.25892286693712...|       3.0|\n",
      "|  0.0|(66314,[0,1,4,30,...|[-974.97171762736...|[0.01269515806936...|       3.0|\n",
      "|  0.0|(66314,[0,1,4,61,...|[-2095.1260825487...|[1.93190453455028...|       2.0|\n",
      "|  0.0|(66314,[0,1,5,8,1...|[-1370.6668856885...|[8.11156938108696...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(66314,[0,1,2,8,4...|[27.7160621412236...|[0.27716062141223...|       0.0|\n",
      "|  0.0|(66314,[0,1,2,12,...|[27.8997586358119...|[0.27899758635811...|       0.0|\n",
      "|  0.0|(66314,[0,1,2,13,...|[27.5742436380678...|[0.27574243638067...|       0.0|\n",
      "|  0.0|(66314,[0,1,2,16,...|[27.6698150036080...|[0.27669815003608...|       0.0|\n",
      "|  0.0|(66314,[0,1,2,25,...|[27.9772419567301...|[0.27977241956730...|       0.0|\n",
      "|  0.0|(66314,[0,1,3,4,1...|[27.6694643212146...|[0.27669464321214...|       0.0|\n",
      "|  0.0|(66314,[0,1,3,5,1...|[28.1452853515514...|[0.28145285351551...|       0.0|\n",
      "|  0.0|(66314,[0,1,3,10,...|[27.1912615929773...|[0.27191261592977...|       0.0|\n",
      "|  0.0|(66314,[0,1,3,31,...|[27.7556923791544...|[0.27755692379154...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,8...|[27.8266771637958...|[0.27826677163795...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,8...|[27.6502092861725...|[0.27650209286172...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[27.7037987430170...|[0.27703798743017...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[27.8855102281537...|[0.27885510228153...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[27.8405702177782...|[0.27840570217778...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,5...|[27.4626079261747...|[0.27462607926174...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,6,1...|[27.5887256167844...|[0.27588725616784...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,12,...|[28.0152810531679...|[0.28015281053167...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,30,...|[27.7812964165460...|[0.27781296416546...|       0.0|\n",
      "|  0.0|(66314,[0,1,4,61,...|[27.7094855735988...|[0.27709485573598...|       0.0|\n",
      "|  0.0|(66314,[0,1,5,8,1...|[27.8376154798239...|[0.27837615479823...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva = MulticlassClassificationEvaluator()\n",
    "acc_NB = eva.evaluate(NB_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva = MulticlassClassificationEvaluator()\n",
    "acc_RF = eva.evaluate(RF_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the NB and RF is :: 0.3910757327772677 0.12258262788347707\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy of the NB and RF is ::\", acc_NB, acc_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
